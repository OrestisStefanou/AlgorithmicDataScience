{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder as a Classifier using Fashion-MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tous kodikes en tous etrexa giati itan na pari para polli ora.Apoti ekatalava en oti zita stin ergasia pano kato.To dataset en polla paromoio ara en nomizo na xriasti na allaksoume je polla "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import gzip\n",
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model,Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(28 * 28 * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(num_images, 28,28)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = extract_data('/home/orestis/EKPA/7examino/AlgorithmicDataScience/Project2/train-images-idx3-ubyte.gz', 60000)\n",
    "test_data = extract_data('/home/orestis/EKPA/7examino/AlgorithmicDataScience/Project2/t10k-images-idx3-ubyte.gz', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = extract_labels('/home/orestis/EKPA/7examino/AlgorithmicDataScience/Project2/train-labels-idx1-ubyte.gz',60000)\n",
    "test_labels = extract_labels('/home/orestis/EKPA/7examino/AlgorithmicDataScience/Project2/t10k-labels-idx1-ubyte.gz',10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration\n",
    "\n",
    "Let's now analyze how images in the dataset look like and also see the dimension of the images with the help of the NumPy array attribute .shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (images) shape: (60000, 28, 28)\n",
      "Test set (images) shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of training set\n",
    "print(\"Training set (images) shape: {shape}\".format(shape=train_data.shape))\n",
    "\n",
    "# Shapes of test set\n",
    "print(\"Test set (images) shape: {shape}\".format(shape=test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, you can see that the training data has a shape of 60000 x 28 x 28 since there are 60,000 training samples each of 28 x 28 dimensional matrix. Similarly, the test data has a shape of 10000 x 28 x 28 since there are 10,000 testing samples.\n",
    "\n",
    "Note that in the task of reconstructing using convolutional autoencoder you won't need training and testing labels. Your training images will both act as the input as well as the ground truth similar to the labels you have in classification task.\n",
    "\n",
    "But for the classification task you will also need your labels along with the images which you will be doing later on in the tutorial. Even though the task at hand will only be dealing with the training and testing images. However, for exploration purposes, which might give you a better intuition about the data, you'll make use of the labels.\n",
    "\n",
    "Let's create a dictionary that will have class names with their corresponding categorical class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of target classes\n",
    "label_dict = {\n",
    " 0: '0',\n",
    " 1: '1',\n",
    " 2: '2',\n",
    " 3: '3',\n",
    " 4: '4',\n",
    " 5: '5',\n",
    " 6: '6',\n",
    " 7: '7',\n",
    " 8: '8',\n",
    " 9: '9',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at a couple of the images in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '(Label: 0)')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "curr_img = np.reshape(train_data[10], (28,28))\n",
    "curr_lbl = train_labels[10]\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "curr_img = np.reshape(test_data[10], (28,28))\n",
    "curr_lbl = test_labels[10]\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n",
    "The images of the dataset are indeed grayscale images with pixel values ranging from 0 to 255 with a dimension of 28 x 28, so before we feed the data into the model, it is very important to preprocess it. You'll first convert each 28 x 28 image of train and test set into a matrix of size 28 x 28 x 1, which you can feed into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.reshape(-1, 28,28, 1)\n",
    "test_data = test_data.reshape(-1, 28,28, 1)\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, rescale the training and testing data with the maximum pixel value of the training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data / np.max(train_data)\n",
    "test_data = test_data / np.max(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of this, it's important to partition the data. In order for our model to generalize well, you split the training data into two parts: a training and a validation set. You will train your model on 80% of the data and validate it on 20% of the remaining training data.\n",
    "\n",
    "This will also help you in reducing the chances of overfitting, as you will be validating your model on data it would not have seen in the training phase.\n",
    "\n",
    "You can use the train_test_split module of scikit-learn to divide the data properly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_ground,valid_ground = train_test_split(train_data,\n",
    "                                                             train_data,\n",
    "                                                             test_size=0.2,\n",
    "                                                             random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You will be using the above data splitting twice, once for the task of reconstructing using convolutional autoencoder for which you don't need training and testing labels. That's why you will pass the training images twice. Your training images will both act as the input as well as the ground truth similar to the labels you have in the classification task.\n",
    "\n",
    "But for the classification task, you will also pass your labels along with the images which you will be doing later on in the tutorial.\n",
    "\n",
    "Now you are all set to define the network and feed the data into the network. So without any further ado, let's jump to the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are of size 28 x 28 x 1 or a 30976-dimensional vector. You convert the image matrix to an array, rescale it between 0 and 1, reshape it so that it's of size 28 x 28 x 1, and feed this as an input to the network.\n",
    "\n",
    "Also, you will use a batch size of 128 using a higher batch size of 256 or 512 is also preferable it all depends on the system you train your model. It contributes heavily in determining the learning parameters and affects the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 35\n",
    "inChannel = 1\n",
    "x, y = 28, 28\n",
    "input_img = Input(shape = (x, y, inChannel))\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might already know well before, the autoencoder is divided into two parts: there's an encoder and a decoder.\n",
    "\n",
    "Encoder: It has 4 Convolution blocks, each block has a convolution layer followed by a batch normalization layer. Max-pooling layer is used after the first and second convolution blocks.\n",
    "\n",
    "    The first convolution block will have 32 filters of size 3 x 3, followed by a downsampling (max-pooling) layer,\n",
    "    The second block will have 64 filters of size 3 x 3, followed by another downsampling layer,\n",
    "    The third block of encoder will have 128 filters of size 3 x 3,\n",
    "    The fourth block of encoder will have 256 filters of size 3 x 3.\n",
    "\n",
    "Decoder: It has 3 Convolution blocks, each block has a convolution layer followed by a batch normalization layer. Upsampling layer is used after the second and third convolution blocks.\n",
    "\n",
    "    The first block will have 128 filters of size 3 x 3,\n",
    "    The second block will have 64 filters of size 3 x 3 followed by another upsampling layer,\n",
    "    The third block will have 32 filters of size 3 x 3 followed by another upsampling layer,\n",
    "    The final layer of encoder will have 1 filter of size 3 x 3 which will reconstruct back the input having a single channel.\n",
    "\n",
    "The max-pooling layer will downsample the input by two times each time you use it, while the upsampling layer will upsample the input by two times each time it is used.\n",
    "\n",
    "Note: The number of filters, the filter size, the number of layers, number of epochs you train your model, are all hyperparameters and should be decided based on your own intuition, you are free to try new experiments by tweaking with these hyperparameters and measure the performance of your model. And that is how you will slowly learn the art of deep learning!\n",
    "\n",
    "Let's create separate encoder and decoder functions since you will be using encoder weights later on for classification purpose!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_img):\n",
    "    #encoder\n",
    "    #input = 28 x 28 x 1 (wide and thin)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small and thick)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    return conv4\n",
    "\n",
    "def decoder(conv4):    \n",
    "    #decoder\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) #7 x 7 x 128\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) #7 x 7 x 64\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is created, you have to compile it using the optimizer to be RMSProp.\n",
    "\n",
    "Note that you also have to specify the loss type via the argument loss. In this case, that's the mean squared error, since the loss after every batch will be computed between the batch of predicted output and the ground truth using mean squared error pixel by pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the layers that you created in the above step by using the summary function. This will show a number of parameters (weights and biases) in each layer and also the total parameters in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 7, 7, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 7, 7, 64)          73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 14, 14, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 1,758,657\n",
      "Trainable params: 1,755,841\n",
      "Non-trainable params: 2,816\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's finally time to train the model with Keras' fit() function! The model trains for 200 epochs. The fit() function will return a history object; By storing the result of this function in autoencoder_train, you can use it later to plot the loss function plot between training and validation which will help you to analyze your model's performance visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "750/750 [==============================] - 1071s 1s/step - loss: 0.0770 - val_loss: 0.0946\n",
      "Epoch 2/35\n",
      "131/750 [====>.........................] - ETA: 14:26 - loss: 0.0579"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a29505c4b436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mautoencoder_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ground\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the loss plot between training and validation data to visualize the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = autoencoder_train.history['loss']\n",
    "val_loss = autoencoder_train.history['val_loss']\n",
    "epochs = range(200)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Model\n",
    "\n",
    "Since you will need the encoder weights in your classification task, first let's save the complete autoencoder weights. You will learn how you can extract the encoder weights soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights('autoencoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmenting the fashion mnist images\n",
    "\n",
    "Now you will be using the trained autoencoder's head, i.e., the encoder part and will be loading the weights of the autoencoder you just now trained but only in the encoder part of the model.\n",
    "\n",
    "You will add a few dense or fully connected layers to the encoder to classify fashion mnist images.\n",
    "\n",
    "    Let's first convert the labels into one-hot encoding vectors.\n",
    "\n",
    "    For those of you who do not know about one-hot encoding:\n",
    "\n",
    "    In one-hot encoding, you convert the categorical data into a vector of numbers. The reason why you convert the categorical data in one hot encoding is that machine learning algorithms cannot work with categorical data directly. You generate one boolean column for each category or class. Only one of these columns could take on the value 1 for each sample. Hence, the term one-hot encoding.\n",
    "\n",
    "    For your problem statement, the one hot encoding will be a row vector, and for each image, it will have a dimension of 1 x 10. The important thing to note here is that the vector consists of all zeros except for the class that it represents, and for that, it is 1.\n",
    "\n",
    "    So let's convert the labels into one-hot encoding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_labels)\n",
    "test_Y_one_hot = to_categorical(test_labels)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_labels[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last step is a crucial one which you all have done before as well which is splitting the data into training and validation in the autoencoder training phase. So let's quickly do that step for classification as well. Note that you will use the same random state that you used before and also this time you will pass your labels as well that you just now converted into a one-hot encoding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,valid_X,train_label,valid_label = train_test_split(train_data,train_Y_one_hot,test_size=0.2,random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the classification model. Remember that you will be using the exact same encoder part as you used in the autoencoder architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the fully connected layers that you will be stacking up with the encoder function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(enco):\n",
    "    flat = Flatten()(enco)\n",
    "    den = Dense(128, activation='relu')(flat)\n",
    "    out = Dense(num_classes, activation='softmax')(den)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = encoder(input_img)\n",
    "full_model = Model(input_img,fc(encode))\n",
    "\n",
    "for l1,l2 in zip(full_model.layers[:19],autoencoder.layers[0:19]):\n",
    "    l1.set_weights(l2.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The next step is pretty important. In order to be sure whether the weights of the encoder part of the autoencoder are similar to the weights you loaded to the encoder function of the classification model, you should always print any one of the same layers weights of both the models. If they are not similar, then there is no use in using the autoencoder classification strategy.\n",
    "\n",
    "Let's print first layer weights of both the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.get_weights()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.get_weights()[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! Both the arrays look exactly similar. So, without any further ado, let's compile the model and start the training.\n",
    "\n",
    "Next, you will make the encoder part i.e.the first nineteen layers of the model trainable false. Since the encoder part is already trained, you do not need to train it. You will only be training the Fully Connected part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in full_model.layers[0:19]:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the summary of the model as well. There should be non-trainable parameters as well since you made the first fifteen layers of the model non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model\n",
    "\n",
    "It's finally time to train the model with Keras' fit() function! The model trains for 10 epochs. The fit() function will return a history object; By storing the result of this function in fashion_train, you can use it later to plot the accuracy and loss function plots between training and validation which will help you to analyze your model's performance visually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_train = full_model.fit(train_X, train_label, batch_size=64,epochs=100,verbose=1,validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save_weights('autoencoder_classification.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, you will re-train the model by making the first nineteen layers trainable as True instead of keeping them False! So, let's quickly do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in full_model.layers[0:19]:\n",
    "    layer.trainable = True\n",
    "\n",
    "    full_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the entire model for one last time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_train = full_model.fit(train_X, train_label, batch_size=64,epochs=100,verbose=1,validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model for one last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save_weights('classification_complete.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put your model evaluation into perspective and plot the accuracy and loss plots between training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = classify_train.history['acc']\n",
    "val_accuracy = classify_train.history['val_acc']\n",
    "loss = classify_train.history['loss']\n",
    "val_loss = classify_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation on the Test Set\n",
    "\n",
    "Finally, let's also evaluate your model on test data and see how it performs!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = full_model.evaluate(test_data, test_Y_one_hot, verbose=0)\n",
    "\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = full_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the predictions you get are floating point values, it will not be feasible to compare the predicted labels with true test labels. So, you will round off the output which will convert the float values into an integer. Further, you will use np.argmax() to select the index number which has a higher value in a row.\n",
    "\n",
    "For example, let's assume a prediction for one test image to be [0 1 0 0 0 0 0 0 0 0], the output for this should be a class label 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
    "predicted_classes.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.where(predicted_classes==test_labels)[0]\n",
    "print \"Found %d correct labels\" % len(correct)\n",
    "for i, correct in enumerate(correct[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_data[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], test_labels[correct]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = np.where(predicted_classes!=test_labels)[0]\n",
    "print \"Found %d incorrect labels\" % len(incorrect)\n",
    "for i, incorrect in enumerate(incorrect[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_data[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], test_labels[incorrect]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report\n",
    "\n",
    "Classification report will help you in identifying the misclassified classes in more detail. You will be able to observe for which class the model performed bad out of the given ten classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
